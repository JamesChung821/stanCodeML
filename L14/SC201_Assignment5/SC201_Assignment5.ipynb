{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSbFti0yUlvj"
   },
   "source": [
    "---\n",
    "# IMPORTANT\n",
    "\n",
    "**Please remember to save this notebook `SC201_Assignment5.ipynb` as you work on it!**\n",
    "\n",
    "### 請大家務必在這份作業中使用 GPU。\n",
    "\n",
    "請點選 `Runtime -> Change runtime type` 並將 `Hardware Accelerator` 設定為 `GPU`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvKceQDgUoy3"
   },
   "outputs": [],
   "source": [
    "# this mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# 請輸入 a5 資料夾之所在位置\n",
    "FOLDERNAME = 'Colab\\ Notebooks/SC201_Assignment5'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# this downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd drive/MyDrive/$FOLDERNAME/sc201/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J4P7Ce9Uoy4",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# What is PyTorch?\n",
    "\n",
    "PyTorch 是一套計算系統，可以用來計算動態圖形 (neural network 是圖形的一種)。這些圖形是由 PyTorch 的 Tensor 物件組成的，Tensor 的用法如同 numpy 矩陣。PyTorch 內建自動微分的功能，使用者就不必手動處理 backward pass！\n",
    "\n",
    "This notebook assumes that you are using **PyTorch version 1.4+**\n",
    "\n",
    "## Why PyTorch?\n",
    "\n",
    "* PyTorch 支援 GPU 計算，我們的 training 就可以利用 GPU 執行，程式會跑的更快！\n",
    "* PyTorch 也是使用 modular design，大家以後就可以直接使用 PyTorch 既有模組（或是自己定義）並隨意拼湊成各式各樣的 neural network！\n",
    "* 學術和業界中的 machine learning 都是使用 PyTorch 或是其他類似的強大計算套件，大家也就能跟上最新的研究和應用！\n",
    "\n",
    "## How can I learn PyTorch on my own?\n",
    "\n",
    "有興趣可以參考網路上的 PyTorch 教學，如 https://github.com/jcjohnson/pytorch-examples \n",
    "\n",
    "另外也可以參考 PyTorch 的說明書 [API doc](http://pytorch.org/docs/stable/index.html)。PyTorch 相關問題會建議大家在 [PyTorch forum](https://discuss.pytorch.org/) 上發問，而非 StackOverflow。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF9UmYilUoy4"
   },
   "source": [
    "# Section I. Preparation\n",
    "\n",
    "大家在之前的作業裡做 data preparation 都是呼叫我們提供的程式。\n",
    "\n",
    "PyTorch 內建的 `DataLoader` 和 `sampler` 類別可以將這個步驟自動化。詳細用法請參考以下的 code，特別是 data 的正規化 (normalization) 和分劃 (partitioning into *train / val / test*)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bL-q1O0mUoy4",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZIFC1x2Uoy4",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./sc201/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./sc201/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./sc201/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HUBOP0GUoy4",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "我們透由 `device` 啟用 PyTorch 的 GPU 功能。\n",
    "\n",
    "（如果您未將 CUDA 開啟，`torch.cuda.is_available()` 會回傳 False，使 notebook 轉回 CPU mode。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0OcEnkAUoy4",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHEQMJjmd7IU"
   },
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss_function = nn.CrossEntropyLoss()\n",
    "            loss = loss_function(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Oa5NtYAezm0"
   },
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh5tqAb7Uoy5"
   },
   "source": [
    "# PyTorch Sequential API\n",
    "\n",
    "### Sequential API: Two-Layer Network\n",
    "以下是 two-layer fully connected network 的 `nn.Sequential` 範例，我們把內建的 layer 依序丟入，並使用同樣的 training loop 進行訓練。\n",
    "\n",
    "大家在這裡不用做 hyperparameter tuning，但是在不做 tuning 的情況下，模型應該還是能在一個 epoch 之內達到 40% 以上的準確率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8qx1q_XUoy5"
   },
   "outputs": [],
   "source": [
    "# We need to wrap `flatten` function in a module in order to stack it\n",
    "# in nn.Sequential\n",
    "\n",
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(3 * 32 * 32, hidden_layer_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 10),\n",
    ")\n",
    "\n",
    "# you can use Nesterov momentum in optim.SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "train_part34(model, optimizer, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjbPzAATUoy5"
   },
   "source": [
    "### Sequential API: Three-Layer ConvNet\n",
    "請大家使用 `nn.Sequential` 建立並訓練出一套 three-layer ConvNet，架構依舊是：\n",
    "\n",
    "1. Convolutional layer (with bias) with 32 5x5 filters, with zero-padding of 2\n",
    "2. ReLU\n",
    "3. Convolutional layer (with bias) with 16 3x3 filters, with zero-padding of 1\n",
    "4. ReLU\n",
    "5. Fully-connected layer (with bias) to compute scores for 10 classes\n",
    "\n",
    "訓練的方式請使用 stochastic gradient descent with Nesterov momentum 0.9。\n",
    "\n",
    "大家在這裡不用做 hyperparameter tuning，但是在不做 tuning 的情況下，模型應該還是能在一個 epoch 之內達到 55% 以上的準確率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sequential_accuracy"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Rewrite the 2-layer ConvNet with bias from Part III with the           #\n",
    "# Sequential API.                                                              #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "pass\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "train_part34(model, optimizer, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edzWXJBshCc2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5IuROyHUoy5"
   },
   "source": [
    "# Section V. CIFAR-10 open-ended challenge\n",
    "\n",
    "最後這個章節是自由發揮題！請大家絞盡腦汁（以及 Google 的 GPU），使用 `nn.Module` 或是 `nn.Sequential` API 設計出一套 CNN 進行訓練，在十個 epoch 之內達到 70% 以上的 CIFAR-10 validation accuracy！上方的 check_accuracy 與 training 函數都可以使用。\n",
    "\n",
    "請參考官方的 API 說明書：\n",
    "\n",
    "* Layers in torch.nn package: http://pytorch.org/docs/stable/nn.html\n",
    "* Activations: http://pytorch.org/docs/stable/nn.html#non-linear-activations\n",
    "* Loss functions: http://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "* Optimizers: http://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SXQJ6s3Ulvr"
   },
   "source": [
    "### Things you might try:\n",
    "- **Filter size**: 上面的 CNN 使用的是 5x5 的 filter。\n",
    "- **Number of filters**: 上面的 filter 數目為 32。\n",
    "- **Pooling vs Strided Convolution**: Max pooling 和 strided convolutions 哪個效果會比較好呢？\n",
    "- **Batch normalization**: 大家可以在 convolution layer 之後附加 spatial batch normalization，affine layer 之後附加 vanilla batch normalization。這樣的網路架構會不會跑得比較快？\n",
    "- **Network architecture**: 深度網路會不會比較強大呢？大家可以試試看：\n",
    "    - [conv-relu-pool] x N -> [affine] x M -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool] x N -> [affine] x M -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv] x N -> [affine] x M -> [softmax or SVM]\n",
    "- **Global Average Pooling**: 一般的 CNN 會在 convolution 結束後做 flattening 然後進入 affine layers。另外一種做法是在 convolution 結束後使用 global average pooling 取得一個 1x1 的 average image（形狀為 (1, 1 , Filter#)），然後 reshape 成長度為 Filter# 的向量。大家可以參考 [Google 的 Inception Network](https://arxiv.org/abs/1512.00567)（see Table 1）。\n",
    "- **Regularization**: 大家可以使用 L2 regularization loss 或是 Dropout。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6g9NBibUlvr"
   },
   "source": [
    "### Tips for training\n",
    "記得要調整 learning rate 等 hyperparameters，找出最好的數值。Tuning 的過程應注意：\n",
    "\n",
    "- 好的 hyperparameter 數值應該在一千個 iteration 以內見效。\n",
    "- 記得使用 coarse-to-fine tuning：\n",
    "    - 先進行粗調，不要訓練太久，不好的 hyperparameter 可以直接略過。\n",
    "    - 找到適當的範圍後再進行微調，訓練更多遍。\n",
    "- Hyperparameter tuning 應該使用 validation set 而不是 test set！後者是留到最後測試最好的模型使用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ja05QaHkUlvs"
   },
   "source": [
    "### Going above and beyond\n",
    "大家如果有興趣，可以自行撰寫程式支援進階的功能！\n",
    "\n",
    "- Alternative optimizers: 使用 Adam、Adagrad、RMSprop 等學習模式。\n",
    "- Alternative activation functions：使用 leaky ReLU、parametric ReLU、ELU、MaxOut 等激勵函數。\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New architectures ([see this blog](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32))\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385)：將前一層的 input 導入下一層。\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993)：將前面所有 layer 的 input 都導入下一層。\n",
    "\n",
    "### Have fun and happy training! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "open_ended_accuracy"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #         \n",
    "# Experiment with any architectures, optimizers, and hyperparameters.          #\n",
    "# Achieve AT LEAST 70% accuracy on the *validation set* within 10 epochs.      #\n",
    "#                                                                              #\n",
    "# Note that you can use the check_accuracy function to evaluate on either      #\n",
    "# the test set or the validation set, by passing either loader_test or         #\n",
    "# loader_val as the second argument to check_accuracy. You should not touch    #\n",
    "# the test set until you have finished your architecture and  hyperparameter   #\n",
    "# tuning, and only run the test set once at the end to report a final value.   #\n",
    "################################################################################\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "# You should get at least 70% accuracy\n",
    "train_part34(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_0UM_JbUoy5",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Describe what you did \n",
    "\n",
    "請敘述您採取的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRfcEubpUoy5",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Answer\n",
    "\n",
    "[FILL THIS IN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Whb7j_oUoy5"
   },
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "請將最好的模型儲存於 `best_model`，並使用 test set 做測試。下方的 test accuracy 跟上方的 validation accuracy 有何關係？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBCQR9IxUoy5"
   },
   "outputs": [],
   "source": [
    "best_model = model\n",
    "check_accuracy_part34(loader_test, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox4bmusHUlvs"
   },
   "source": [
    "---\n",
    "# IMPORTANT\n",
    "\n",
    "恭喜大家完成作業！**請開啟資料夾的分享功能，並將共用連結填寫在 stanCode 作業繳交表單內！**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SC201_Assignment5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
