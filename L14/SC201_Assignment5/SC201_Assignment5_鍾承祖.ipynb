{"cells":[{"cell_type":"markdown","metadata":{"id":"gSbFti0yUlvj"},"source":["---\n","# IMPORTANT\n","\n","**Please remember to save this notebook `SC201_Assignment5.ipynb` as you work on it!**\n","\n","### 請大家務必在這份作業中使用 GPU。\n","\n","請點選 `Runtime -> Change runtime type` 並將 `Hardware Accelerator` 設定為 `GPU`。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37296,"status":"ok","timestamp":1647781906633,"user":{"displayName":"Cheng-Chu Chung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjjk6SqeMpTHVe4ITb3MezIB6-5WeNNKG3w1LsD=s64","userId":"17774558080259118332"},"user_tz":240},"id":"UvKceQDgUoy3","outputId":"0d0ec515-620f-4ce5-f50a-a7c5460f47e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/Othercomputers/我的 筆記型電腦 MSI/Research data/stanCodeML/L14/SC201_Assignment5/sc201/datasets\n","/content\n"]}],"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# 請輸入 a5 資料夾之所在位置\n","FOLDERNAME = '我的 筆記型電腦 MSI/Research data/stanCodeML/L14/SC201_Assignment5'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/Othercomputers/{}'.format(FOLDERNAME))\n","# /content/drive/Othercomputers/我的 筆記型電腦 MSI/Research data/stanCodeML/L14/SC201_Assignment5/SC201_Assignment5.ipynb\n","# this downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist.\n","%cd drive/Othercomputers/$FOLDERNAME/sc201/datasets/\n","!bash get_datasets.sh\n","%cd /content"]},{"cell_type":"markdown","metadata":{"id":"0J4P7Ce9Uoy4"},"source":["# What is PyTorch?\n","\n","PyTorch 是一套計算系統，可以用來計算動態圖形 (neural network 是圖形的一種)。這些圖形是由 PyTorch 的 Tensor 物件組成的，Tensor 的用法如同 numpy 矩陣。PyTorch 內建自動微分的功能，使用者就不必手動處理 backward pass！\n","\n","This notebook assumes that you are using **PyTorch version 1.4+**\n","\n","## Why PyTorch?\n","\n","* PyTorch 支援 GPU 計算，我們的 training 就可以利用 GPU 執行，程式會跑的更快！\n","* PyTorch 也是使用 modular design，大家以後就可以直接使用 PyTorch 既有模組（或是自己定義）並隨意拼湊成各式各樣的 neural network！\n","* 學術和業界中的 machine learning 都是使用 PyTorch 或是其他類似的強大計算套件，大家也就能跟上最新的研究和應用！\n","\n","## How can I learn PyTorch on my own?\n","\n","有興趣可以參考網路上的 PyTorch 教學，如 https://github.com/jcjohnson/pytorch-examples \n","\n","另外也可以參考 PyTorch 的說明書 [API doc](http://pytorch.org/docs/stable/index.html)。PyTorch 相關問題會建議大家在 [PyTorch forum](https://discuss.pytorch.org/) 上發問，而非 StackOverflow。"]},{"cell_type":"markdown","metadata":{"id":"RF9UmYilUoy4"},"source":["# Section I. Preparation\n","\n","大家在之前的作業裡做 data preparation 都是呼叫我們提供的程式。\n","\n","PyTorch 內建的 `DataLoader` 和 `sampler` 類別可以將這個步驟自動化。詳細用法請參考以下的 code，特別是 data 的正規化 (normalization) 和分劃 (partitioning into *train / val / test*)。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bL-q1O0mUoy4"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.utils.data import sampler\n","\n","import torchvision.datasets as dset\n","import torchvision.transforms as T\n","\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["b20f9c7289114ad0a97fa08da85a1b3f","db2710121d6041a0aa858638597f80a2","5268755a9d794851b4cea7134a999f89","f9af134660944fc881ff246775895183","507c9df52a7d49fa8061bd535aeb17da","efacad7a4b7246b7a24e0a30a4228c4e","e0ea73b3be66493a9afd107f8f6948b7","6bce86c0fa37495ea220c47a18319cd9","f7d0d54e1f60467c8d4970a7024b1bef","e349421b0042416e9a70e81fbf222eaf","214ddc8c2da744839c06ec3c370a780f"]},"executionInfo":{"elapsed":19434,"status":"ok","timestamp":1647781932171,"user":{"displayName":"Cheng-Chu Chung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjjk6SqeMpTHVe4ITb3MezIB6-5WeNNKG3w1LsD=s64","userId":"17774558080259118332"},"user_tz":240},"id":"NZIFC1x2Uoy4","outputId":"3de8acf9-5e6f-4ab3-e9ab-fe3872aed8b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./sc201/datasets/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b20f9c7289114ad0a97fa08da85a1b3f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./sc201/datasets/cifar-10-python.tar.gz to ./sc201/datasets\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Total number of the data: 50000\n"]}],"source":["from torchvision.transforms.transforms import RandomHorizontalFlip\n","NUM_TRAIN = 49000\n","\n","# The torchvision.transforms package provides tools for preprocessing data\n","# and for performing data augmentation; here we set up a transform to\n","# preprocess the data by subtracting the mean RGB value and dividing by the\n","# standard deviation of each RGB value; we've hardcoded the mean and std.\n","transform = T.Compose([\n","                T.ToTensor(),\n","                # T.RandomHorizontalFlip(p=0.5),\n","                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","            ]) # (mean), (std)\n","\n","# We set up a Dataset object for each split (train / val / test); Datasets load\n","# training examples one at a time, so we wrap each Dataset in a DataLoader which\n","# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n","# training set into train and val sets by passing a Sampler object to the\n","# DataLoader telling how it should sample from the underlying Dataset.\n","cifar10_train = dset.CIFAR10('./sc201/datasets', train=True, download=True,\n","                             transform=transform)\n","loader_train = DataLoader(cifar10_train, batch_size=64, \n","                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n","\n","cifar10_val = dset.CIFAR10('./sc201/datasets', train=True, download=True,\n","                           transform=transform)\n","loader_val = DataLoader(cifar10_val, batch_size=64, \n","                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n","\n","cifar10_test = dset.CIFAR10('./sc201/datasets', train=False, download=True, \n","                            transform=transform)\n","loader_test = DataLoader(cifar10_test, batch_size=64)\n","\n","print('Total number of the data:', len(cifar10_train))"]},{"cell_type":"markdown","metadata":{"id":"8HUBOP0GUoy4"},"source":["我們透由 `device` 啟用 PyTorch 的 GPU 功能。\n","\n","（如果您未將 CUDA 開啟，`torch.cuda.is_available()` 會回傳 False，使 notebook 轉回 CPU mode。）"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1647781932172,"user":{"displayName":"Cheng-Chu Chung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjjk6SqeMpTHVe4ITb3MezIB6-5WeNNKG3w1LsD=s64","userId":"17774558080259118332"},"user_tz":240},"id":"T0OcEnkAUoy4","outputId":"1d422597-6108-44f5-ccf1-74ae02868bf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["using device: cuda\n"]}],"source":["USE_GPU = True\n","\n","dtype = torch.float32 # we will be using float throughout this tutorial\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","# Constant to control how frequently we print train loss\n","print_every = 100\n","\n","print('using device:', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHEQMJjmd7IU"},"outputs":[],"source":["def train_part34(model, optimizer, epochs=1):\n","    \"\"\"\n","    Train a model on CIFAR-10 using the PyTorch Module API.\n","    \n","    Inputs:\n","    - model: A PyTorch Module giving the model to train.\n","    - optimizer: An Optimizer object we will use to train the model\n","    - epochs: (Optional) A Python integer giving the number of epochs to train for\n","    \n","    Returns: Nothing, but prints model accuracies during training.\n","    \"\"\"\n","    model = model.to(device=device)  # move the model parameters to CPU/GPU\n","    for e in range(epochs):\n","      print('----------------------------Number of epochs:', e+1)\n","      for t, (x, y) in enumerate(loader_train):\n","          model.train()  # put model to training mode\n","          x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n","          y = y.to(device=device, dtype=torch.long) # tensor is index\n","\n","          scores = model(x)\n","          loss_function = nn.CrossEntropyLoss()\n","          loss = loss_function(scores, y)\n","\n","          # Zero out all of the gradients for the variables which the optimizer\n","          # will update.\n","          optimizer.zero_grad()\n","\n","          # This is the backwards pass: compute the gradient of the loss with\n","          # respect to each  parameter of the model.\n","          loss.backward()\n","\n","          # Actually update the parameters of the model using the gradients\n","          # computed by the backwards pass.\n","          optimizer.step()\n","\n","          if t % print_every == 0:\n","              print('Iteration %d, loss = %.4f' % (t, loss.item())) # d is decimal integer\n","              check_accuracy_part34(loader_val, model)\n","              print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Oa5NtYAezm0"},"outputs":[],"source":["def check_accuracy_part34(loader, model):\n","    if loader.dataset.train:\n","        print('Checking accuracy on validation set')\n","    else:\n","        print('Checking accuracy on test set')   \n","    num_correct = 0\n","    num_samples = 0\n","    model.eval()  # set model to evaluation mode\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n","            y = y.to(device=device, dtype=torch.long)\n","            scores = model(x)\n","            _, preds = scores.max(1) # dimension\n","            \"\"\"Returns a namedtuple (values, indices) \n","            where values is the maximum value of each row \n","            of the input tensor in the given dimension dim. \n","            And indices is the index location \n","            of each maximum value found (argmax).\"\"\"\n","            num_correct += (preds == y).sum()\n","            num_samples += preds.size(0)\n","        acc = float(num_correct) / num_samples\n","        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"]},{"cell_type":"markdown","metadata":{"id":"Zh5tqAb7Uoy5"},"source":["# PyTorch Sequential API\n","\n","### Sequential API: Two-Layer Network\n","以下是 two-layer fully connected network 的 `nn.Sequential` 範例，我們把內建的 layer 依序丟入，並使用同樣的 training loop 進行訓練。\n","\n","大家在這裡不用做 hyperparameter tuning，但是在不做 tuning 的情況下，模型應該還是能在一個 epoch 之內達到 40% 以上的準確率。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21916,"status":"ok","timestamp":1647781954080,"user":{"displayName":"Cheng-Chu Chung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjjk6SqeMpTHVe4ITb3MezIB6-5WeNNKG3w1LsD=s64","userId":"17774558080259118332"},"user_tz":240},"id":"S8qx1q_XUoy5","outputId":"94219a43-9403-45c8-8286-e2466845a5b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------Number of epochs: 1\n","Iteration 0, loss = 2.3932\n","Checking accuracy on validation set\n","Got 176 / 1000 correct (17.60)\n","\n","Iteration 100, loss = 1.5128\n","Checking accuracy on validation set\n","Got 370 / 1000 correct (37.00)\n","\n","Iteration 200, loss = 1.7574\n","Checking accuracy on validation set\n","Got 414 / 1000 correct (41.40)\n","\n","Iteration 300, loss = 2.0008\n","Checking accuracy on validation set\n","Got 434 / 1000 correct (43.40)\n","\n","Iteration 400, loss = 1.8535\n","Checking accuracy on validation set\n","Got 428 / 1000 correct (42.80)\n","\n","Iteration 500, loss = 1.5506\n","Checking accuracy on validation set\n","Got 397 / 1000 correct (39.70)\n","\n","Iteration 600, loss = 1.8976\n","Checking accuracy on validation set\n","Got 442 / 1000 correct (44.20)\n","\n","Iteration 700, loss = 1.6946\n","Checking accuracy on validation set\n","Got 464 / 1000 correct (46.40)\n","\n"]}],"source":["# We need to wrap `flatten` function in a module in order to stack it\n","# in nn.Sequential\n","\n","hidden_layer_size = 4000\n","learning_rate = 1e-2\n","\n","model = nn.Sequential(\n","    nn.Flatten(),\n","    nn.Linear(3 * 32 * 32, hidden_layer_size),\n","    nn.ReLU(),\n","    nn.Linear(hidden_layer_size, 10),\n",")\n","\n","# you can use Nesterov momentum in optim.SGD\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n","                     momentum=0.9, nesterov=True)\n","\n","train_part34(model, optimizer, epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"FjbPzAATUoy5"},"source":["### Sequential API: Three-Layer ConvNet\n","請大家使用 `nn.Sequential` 建立並訓練出一套 three-layer ConvNet，架構依舊是：\n","\n","1. Convolutional layer (with bias) with 32 5x5 filters, with zero-padding of 2\n","2. ReLU\n","3. Convolutional layer (with bias) with 16 3x3 filters, with zero-padding of 1\n","4. ReLU\n","5. Fully-connected layer (with bias) to compute scores for 10 classes\n","\n","訓練的方式請使用 stochastic gradient descent with Nesterov momentum 0.9。\n","\n","大家在這裡不用做 hyperparameter tuning，但是在不做 tuning 的情況下，模型應該還是能在一個 epoch 之內達到 55% 以上的準確率。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34631,"status":"ok","timestamp":1647781988702,"user":{"displayName":"Cheng-Chu Chung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjjk6SqeMpTHVe4ITb3MezIB6-5WeNNKG3w1LsD=s64","userId":"17774558080259118332"},"user_tz":240},"id":"sequential_accuracy","outputId":"f9ec949f-f67a-4434-f91b-423821da543c"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------Number of epochs: 1\n","Iteration 0, loss = 2.2998\n","Checking accuracy on validation set\n","Got 120 / 1000 correct (12.00)\n","\n","Iteration 100, loss = 1.4314\n","Checking accuracy on validation set\n","Got 514 / 1000 correct (51.40)\n","\n","Iteration 200, loss = 1.3789\n","Checking accuracy on validation set\n","Got 553 / 1000 correct (55.30)\n","\n","Iteration 300, loss = 1.2837\n","Checking accuracy on validation set\n","Got 596 / 1000 correct (59.60)\n","\n","Iteration 400, loss = 1.1367\n","Checking accuracy on validation set\n","Got 614 / 1000 correct (61.40)\n","\n","Iteration 500, loss = 1.3291\n","Checking accuracy on validation set\n","Got 641 / 1000 correct (64.10)\n","\n","Iteration 600, loss = 0.9429\n","Checking accuracy on validation set\n","Got 623 / 1000 correct (62.30)\n","\n","Iteration 700, loss = 0.7880\n","Checking accuracy on validation set\n","Got 654 / 1000 correct (65.40)\n","\n","----------------------------Number of epochs: 2\n","Iteration 0, loss = 0.8172\n","Checking accuracy on validation set\n","Got 647 / 1000 correct (64.70)\n","\n","Iteration 100, loss = 0.8276\n","Checking accuracy on validation set\n","Got 668 / 1000 correct (66.80)\n","\n","Iteration 200, loss = 0.9733\n","Checking accuracy on validation set\n","Got 646 / 1000 correct (64.60)\n","\n","Iteration 300, loss = 0.9503\n","Checking accuracy on validation set\n","Got 667 / 1000 correct (66.70)\n","\n","Iteration 400, loss = 0.9135\n","Checking accuracy on validation set\n","Got 675 / 1000 correct (67.50)\n","\n","Iteration 500, loss = 1.0624\n","Checking accuracy on validation set\n","Got 684 / 1000 correct (68.40)\n","\n","Iteration 600, loss = 0.9018\n","Checking accuracy on validation set\n","Got 689 / 1000 correct (68.90)\n","\n","Iteration 700, loss = 0.6255\n","Checking accuracy on validation set\n","Got 693 / 1000 correct (69.30)\n","\n","----------------------------Number of epochs: 3\n","Iteration 0, loss = 0.9890\n","Checking accuracy on validation set\n","Got 706 / 1000 correct (70.60)\n","\n","Iteration 100, loss = 0.8634\n","Checking accuracy on validation set\n","Got 691 / 1000 correct (69.10)\n","\n","Iteration 200, loss = 0.7334\n","Checking accuracy on validation set\n","Got 715 / 1000 correct (71.50)\n","\n","Iteration 300, loss = 0.5832\n","Checking accuracy on validation set\n","Got 689 / 1000 correct (68.90)\n","\n","Iteration 400, loss = 0.5622\n","Checking accuracy on validation set\n","Got 701 / 1000 correct (70.10)\n","\n","Iteration 500, loss = 0.8367\n","Checking accuracy on validation set\n","Got 721 / 1000 correct (72.10)\n","\n","Iteration 600, loss = 0.8532\n","Checking accuracy on validation set\n","Got 707 / 1000 correct (70.70)\n","\n","Iteration 700, loss = 0.8840\n","Checking accuracy on validation set\n","Got 730 / 1000 correct (73.00)\n","\n"]}],"source":["model = None\n","optimizer = None\n","\n","################################################################################\n","# TODO: Rewrite the 2-layer ConvNet with bias from Part III with the           #\n","# Sequential API.                                                              #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","# model = nn.Sequential(\n","#     # Your Model Here\n","#     # N * 3 * 32 * 32\n","#     nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2),\n","#     nn.BatchNorm2d(32),\n","#     nn.ReLU(),\n","#     nn.MaxPool2d(kernel_size=2, stride=2),\n","#     # N * 32 * 16 * 16\n","#     nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1),\n","#     nn.BatchNorm2d(16),\n","#     nn.ReLU(),\n","#     nn.MaxPool2d(2, 2),\n","#     # N * 16 * 8 * 8\n","#     nn.Flatten(),\n","#     nn.Linear(in_features=16*8*8, out_features=10)\n","# )\n","class MyCNN(nn.Module):\n","  def __init__(self) -> None:\n","      super().__init__()\n","      self.conv1 = nn.Conv2d(3, 64, 3, 1, 1)\n","      self.conv2 = nn.Conv2d(64, 128, 3, 1, 1)\n","      self.fc = nn.Linear(128*8*8, 10)\n","  def forward(self, x):\n","    x = self.conv1(x)\n","    x = F.relu(x)\n","    x = F.max_pool2d(x, (2, 2))\n","    x = self.conv2(x)\n","    x = F.relu(x)\n","    x = F.max_pool2d(x, (2, 2))\n","    x = torch.flatten(x, 1)\n","    return self.fc(x)\n","\n","model = MyCNN()\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n","                     momentum=0.9, nesterov=True)\n","# optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             \n","################################################################################\n","\n","train_part34(model, optimizer, epochs=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edzWXJBshCc2"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"n5IuROyHUoy5"},"source":["# Section V. CIFAR-10 open-ended challenge\n","\n","最後這個章節是自由發揮題！請大家絞盡腦汁（以及 Google 的 GPU），使用 `nn.Module` 或是 `nn.Sequential` API 設計出一套 CNN 進行訓練，在十個 epoch 之內達到 70% 以上的 CIFAR-10 validation accuracy！上方的 check_accuracy 與 training 函數都可以使用。\n","\n","請參考官方的 API 說明書：\n","\n","* Layers in torch.nn package: http://pytorch.org/docs/stable/nn.html\n","* Activations: http://pytorch.org/docs/stable/nn.html#non-linear-activations\n","* Loss functions: http://pytorch.org/docs/stable/nn.html#loss-functions\n","* Optimizers: http://pytorch.org/docs/stable/optim.html"]},{"cell_type":"markdown","metadata":{"id":"0SXQJ6s3Ulvr"},"source":["### Things you might try:\n","- **Filter size**: 上面的 CNN 使用的是 5x5 的 filter。\n","- **Number of filters**: 上面的 filter 數目為 32。\n","- **Pooling vs Strided Convolution**: Max pooling 和 strided convolutions 哪個效果會比較好呢？\n","- **Batch normalization**: 大家可以在 convolution layer 之後附加 spatial batch normalization，affine layer 之後附加 vanilla batch normalization。這樣的網路架構會不會跑得比較快？\n","- **Network architecture**: 深度網路會不會比較強大呢？大家可以試試看：\n","    - [conv-relu-pool] x N -> [affine] x M -> [softmax or SVM]\n","    - [conv-relu-conv-relu-pool] x N -> [affine] x M -> [softmax or SVM]\n","    - [batchnorm-relu-conv] x N -> [affine] x M -> [softmax or SVM]\n","- **Global Average Pooling**: 一般的 CNN 會在 convolution 結束後做 flattening 然後進入 affine layers。另外一種做法是在 convolution 結束後使用 global average pooling 取得一個 1x1 的 average image（形狀為 (1, 1 , Filter#)），然後 reshape 成長度為 Filter# 的向量。大家可以參考 [Google 的 Inception Network](https://arxiv.org/abs/1512.00567)（see Table 1）。\n","- **Regularization**: 大家可以使用 L2 regularization loss 或是 Dropout。"]},{"cell_type":"markdown","metadata":{"id":"Y6g9NBibUlvr"},"source":["### Tips for training\n","記得要調整 learning rate 等 hyperparameters，找出最好的數值。Tuning 的過程應注意：\n","\n","- 好的 hyperparameter 數值應該在一千個 iteration 以內見效。\n","- 記得使用 coarse-to-fine tuning：\n","    - 先進行粗調，不要訓練太久，不好的 hyperparameter 可以直接略過。\n","    - 找到適當的範圍後再進行微調，訓練更多遍。\n","- Hyperparameter tuning 應該使用 validation set 而不是 test set！後者是留到最後測試最好的模型使用的。"]},{"cell_type":"markdown","metadata":{"id":"ja05QaHkUlvs"},"source":["### Going above and beyond\n","大家如果有興趣，可以自行撰寫程式支援進階的功能！\n","\n","- Alternative optimizers: 使用 Adam、Adagrad、RMSprop 等學習模式。\n","- Alternative activation functions：使用 leaky ReLU、parametric ReLU、ELU、MaxOut 等激勵函數。\n","- Model ensembles\n","- Data augmentation\n","- New architectures ([see this blog](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32))\n","  - [ResNets](https://arxiv.org/abs/1512.03385)：將前一層的 input 導入下一層。\n","  - [DenseNets](https://arxiv.org/abs/1608.06993)：將前面所有 layer 的 input 都導入下一層。\n","\n","### Have fun and happy training! "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"open_ended_accuracy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647784584066,"user_tz":240,"elapsed":120794,"user":{"displayName":"Cheng-Chu Chung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjjk6SqeMpTHVe4ITb3MezIB6-5WeNNKG3w1LsD=s64","userId":"17774558080259118332"}},"outputId":"2bdd053f-36c8-4609-f82a-c1fb06916d4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------Number of epochs: 1\n","Iteration 0, loss = 2.4677\n","Checking accuracy on validation set\n","Got 118 / 1000 correct (11.80)\n","\n","Iteration 100, loss = 1.8249\n","Checking accuracy on validation set\n","Got 432 / 1000 correct (43.20)\n","\n","Iteration 200, loss = 1.3821\n","Checking accuracy on validation set\n","Got 520 / 1000 correct (52.00)\n","\n","Iteration 300, loss = 1.2878\n","Checking accuracy on validation set\n","Got 517 / 1000 correct (51.70)\n","\n","Iteration 400, loss = 1.2537\n","Checking accuracy on validation set\n","Got 567 / 1000 correct (56.70)\n","\n","Iteration 500, loss = 1.0419\n","Checking accuracy on validation set\n","Got 566 / 1000 correct (56.60)\n","\n","Iteration 600, loss = 1.1310\n","Checking accuracy on validation set\n","Got 564 / 1000 correct (56.40)\n","\n","Iteration 700, loss = 0.8799\n","Checking accuracy on validation set\n","Got 629 / 1000 correct (62.90)\n","\n","----------------------------Number of epochs: 2\n","Iteration 0, loss = 1.0938\n","Checking accuracy on validation set\n","Got 625 / 1000 correct (62.50)\n","\n","Iteration 100, loss = 1.1972\n","Checking accuracy on validation set\n","Got 615 / 1000 correct (61.50)\n","\n","Iteration 200, loss = 0.9990\n","Checking accuracy on validation set\n","Got 635 / 1000 correct (63.50)\n","\n","Iteration 300, loss = 0.9144\n","Checking accuracy on validation set\n","Got 655 / 1000 correct (65.50)\n","\n","Iteration 400, loss = 0.8392\n","Checking accuracy on validation set\n","Got 647 / 1000 correct (64.70)\n","\n","Iteration 500, loss = 0.8357\n","Checking accuracy on validation set\n","Got 641 / 1000 correct (64.10)\n","\n","Iteration 600, loss = 0.8182\n","Checking accuracy on validation set\n","Got 621 / 1000 correct (62.10)\n","\n","Iteration 700, loss = 1.0894\n","Checking accuracy on validation set\n","Got 631 / 1000 correct (63.10)\n","\n","----------------------------Number of epochs: 3\n","Iteration 0, loss = 1.0492\n","Checking accuracy on validation set\n","Got 642 / 1000 correct (64.20)\n","\n","Iteration 100, loss = 0.7286\n","Checking accuracy on validation set\n","Got 623 / 1000 correct (62.30)\n","\n","Iteration 200, loss = 0.9372\n","Checking accuracy on validation set\n","Got 690 / 1000 correct (69.00)\n","\n","Iteration 300, loss = 0.8729\n","Checking accuracy on validation set\n","Got 664 / 1000 correct (66.40)\n","\n","Iteration 400, loss = 0.8803\n","Checking accuracy on validation set\n","Got 697 / 1000 correct (69.70)\n","\n","Iteration 500, loss = 0.7946\n","Checking accuracy on validation set\n","Got 684 / 1000 correct (68.40)\n","\n","Iteration 600, loss = 0.6820\n","Checking accuracy on validation set\n","Got 660 / 1000 correct (66.00)\n","\n","Iteration 700, loss = 0.7790\n","Checking accuracy on validation set\n","Got 684 / 1000 correct (68.40)\n","\n","----------------------------Number of epochs: 4\n","Iteration 0, loss = 0.8061\n","Checking accuracy on validation set\n","Got 672 / 1000 correct (67.20)\n","\n","Iteration 100, loss = 0.6900\n","Checking accuracy on validation set\n","Got 691 / 1000 correct (69.10)\n","\n","Iteration 200, loss = 0.8907\n","Checking accuracy on validation set\n","Got 708 / 1000 correct (70.80)\n","\n","Iteration 300, loss = 0.6079\n","Checking accuracy on validation set\n","Got 702 / 1000 correct (70.20)\n","\n","Iteration 400, loss = 0.7839\n","Checking accuracy on validation set\n","Got 690 / 1000 correct (69.00)\n","\n","Iteration 500, loss = 0.8964\n","Checking accuracy on validation set\n","Got 693 / 1000 correct (69.30)\n","\n","Iteration 600, loss = 0.9258\n","Checking accuracy on validation set\n","Got 703 / 1000 correct (70.30)\n","\n","Iteration 700, loss = 0.5617\n","Checking accuracy on validation set\n","Got 683 / 1000 correct (68.30)\n","\n","----------------------------Number of epochs: 5\n","Iteration 0, loss = 0.6896\n","Checking accuracy on validation set\n","Got 722 / 1000 correct (72.20)\n","\n","Iteration 100, loss = 0.8985\n","Checking accuracy on validation set\n","Got 699 / 1000 correct (69.90)\n","\n","Iteration 200, loss = 0.8579\n","Checking accuracy on validation set\n","Got 706 / 1000 correct (70.60)\n","\n","Iteration 300, loss = 0.7140\n","Checking accuracy on validation set\n","Got 725 / 1000 correct (72.50)\n","\n","Iteration 400, loss = 0.8876\n","Checking accuracy on validation set\n","Got 689 / 1000 correct (68.90)\n","\n","Iteration 500, loss = 0.6833\n","Checking accuracy on validation set\n","Got 719 / 1000 correct (71.90)\n","\n","Iteration 600, loss = 0.7701\n","Checking accuracy on validation set\n","Got 695 / 1000 correct (69.50)\n","\n","Iteration 700, loss = 1.1165\n","Checking accuracy on validation set\n","Got 723 / 1000 correct (72.30)\n","\n","----------------------------Number of epochs: 6\n","Iteration 0, loss = 0.6215\n","Checking accuracy on validation set\n","Got 716 / 1000 correct (71.60)\n","\n","Iteration 100, loss = 0.6574\n","Checking accuracy on validation set\n","Got 705 / 1000 correct (70.50)\n","\n","Iteration 200, loss = 0.7980\n","Checking accuracy on validation set\n","Got 734 / 1000 correct (73.40)\n","\n","Iteration 300, loss = 0.5147\n","Checking accuracy on validation set\n","Got 721 / 1000 correct (72.10)\n","\n","Iteration 400, loss = 0.5043\n","Checking accuracy on validation set\n","Got 716 / 1000 correct (71.60)\n","\n","Iteration 500, loss = 0.8502\n","Checking accuracy on validation set\n","Got 729 / 1000 correct (72.90)\n","\n","Iteration 600, loss = 0.8018\n","Checking accuracy on validation set\n","Got 725 / 1000 correct (72.50)\n","\n","Iteration 700, loss = 0.6685\n","Checking accuracy on validation set\n","Got 715 / 1000 correct (71.50)\n","\n","----------------------------Number of epochs: 7\n","Iteration 0, loss = 0.8950\n","Checking accuracy on validation set\n","Got 742 / 1000 correct (74.20)\n","\n","Iteration 100, loss = 0.5970\n","Checking accuracy on validation set\n","Got 719 / 1000 correct (71.90)\n","\n","Iteration 200, loss = 0.6176\n","Checking accuracy on validation set\n","Got 747 / 1000 correct (74.70)\n","\n","Iteration 300, loss = 0.6483\n","Checking accuracy on validation set\n","Got 742 / 1000 correct (74.20)\n","\n","Iteration 400, loss = 0.8320\n","Checking accuracy on validation set\n","Got 697 / 1000 correct (69.70)\n","\n","Iteration 500, loss = 0.4308\n","Checking accuracy on validation set\n","Got 727 / 1000 correct (72.70)\n","\n","Iteration 600, loss = 0.6662\n","Checking accuracy on validation set\n","Got 743 / 1000 correct (74.30)\n","\n","Iteration 700, loss = 0.6501\n","Checking accuracy on validation set\n","Got 746 / 1000 correct (74.60)\n","\n","----------------------------Number of epochs: 8\n","Iteration 0, loss = 0.7450\n","Checking accuracy on validation set\n","Got 714 / 1000 correct (71.40)\n","\n","Iteration 100, loss = 0.9144\n","Checking accuracy on validation set\n","Got 725 / 1000 correct (72.50)\n","\n","Iteration 200, loss = 0.4465\n","Checking accuracy on validation set\n","Got 709 / 1000 correct (70.90)\n","\n","Iteration 300, loss = 0.7686\n","Checking accuracy on validation set\n","Got 732 / 1000 correct (73.20)\n","\n","Iteration 400, loss = 0.6622\n","Checking accuracy on validation set\n","Got 731 / 1000 correct (73.10)\n","\n","Iteration 500, loss = 0.8578\n","Checking accuracy on validation set\n","Got 732 / 1000 correct (73.20)\n","\n","Iteration 600, loss = 0.8350\n","Checking accuracy on validation set\n","Got 736 / 1000 correct (73.60)\n","\n","Iteration 700, loss = 0.7293\n","Checking accuracy on validation set\n","Got 755 / 1000 correct (75.50)\n","\n","----------------------------Number of epochs: 9\n","Iteration 0, loss = 0.5642\n","Checking accuracy on validation set\n","Got 763 / 1000 correct (76.30)\n","\n","Iteration 100, loss = 0.7199\n","Checking accuracy on validation set\n","Got 752 / 1000 correct (75.20)\n","\n","Iteration 200, loss = 0.6244\n","Checking accuracy on validation set\n","Got 734 / 1000 correct (73.40)\n","\n","Iteration 300, loss = 1.0870\n","Checking accuracy on validation set\n","Got 753 / 1000 correct (75.30)\n","\n","Iteration 400, loss = 0.6803\n","Checking accuracy on validation set\n","Got 748 / 1000 correct (74.80)\n","\n","Iteration 500, loss = 0.7172\n","Checking accuracy on validation set\n","Got 742 / 1000 correct (74.20)\n","\n","Iteration 600, loss = 0.6938\n","Checking accuracy on validation set\n","Got 754 / 1000 correct (75.40)\n","\n","Iteration 700, loss = 0.7924\n","Checking accuracy on validation set\n","Got 749 / 1000 correct (74.90)\n","\n","----------------------------Number of epochs: 10\n","Iteration 0, loss = 0.4939\n","Checking accuracy on validation set\n","Got 723 / 1000 correct (72.30)\n","\n","Iteration 100, loss = 0.5263\n","Checking accuracy on validation set\n","Got 722 / 1000 correct (72.20)\n","\n","Iteration 200, loss = 0.5308\n","Checking accuracy on validation set\n","Got 744 / 1000 correct (74.40)\n","\n","Iteration 300, loss = 0.6615\n","Checking accuracy on validation set\n","Got 749 / 1000 correct (74.90)\n","\n","Iteration 400, loss = 0.5763\n","Checking accuracy on validation set\n","Got 770 / 1000 correct (77.00)\n","\n","Iteration 500, loss = 0.7740\n","Checking accuracy on validation set\n","Got 737 / 1000 correct (73.70)\n","\n","Iteration 600, loss = 0.7980\n","Checking accuracy on validation set\n","Got 733 / 1000 correct (73.30)\n","\n","Iteration 700, loss = 0.5544\n","Checking accuracy on validation set\n","Got 720 / 1000 correct (72.00)\n","\n"]}],"source":["from torch.nn.modules.activation import Softmax\n","################################################################################\n","# TODO:                                                                        #         \n","# Experiment with any architectures, optimizers, and hyperparameters.          #\n","# Achieve AT LEAST 70% accuracy on the *validation set* within 10 epochs.      #\n","#                                                                              #\n","# Note that you can use the check_accuracy function to evaluate on either      #\n","# the test set or the validation set, by passing either loader_test or         #\n","# loader_val as the second argument to check_accuracy. You should not touch    #\n","# the test set until you have finished your architecture and  hyperparameter   #\n","# tuning, and only run the test set once at the end to report a final value.   #\n","################################################################################\n","model = None\n","optimizer = None\n","learning_rate = 1e-2\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","model = nn.Sequential(\n","    # Your Model Here\n","    # N * 3 (number of filters) * 32 (input size) * 32 (input size)\n","    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n","    nn.BatchNorm2d(64),\n","    nn.ReLU(),\n","    # nn.MaxPool2d(kernel_size=2, stride=2),\n","    nn.AvgPool2d(2, 2),\n","\n","    # N * 64 * 16 * 16\n","    nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1),\n","    nn.BatchNorm2d(32),\n","    nn.ReLU(),\n","    # nn.MaxPool2d(2, 2),\n","    nn.AvgPool2d(2, 2),\n","\n","    # N * 32 * 8 * 8\n","    nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1),\n","    nn.BatchNorm2d(16),\n","    nn.ReLU(),\n","    # nn.MaxPool2d(2, 2),\n","    nn.AvgPool2d(2, 2),\n","\n","    # N * 16 * 4 * 4\n","    nn.Flatten(),\n","    nn.Linear(in_features=16 * 4 * 4, out_features=10),\n","    # nn.Linear(in_features=100, out_features=10),\n","    # nn.Softmax(dim=1)\n",")\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n","                     momentum=0.9, nesterov=True)\n","# optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                                 END OF YOUR CODE                             \n","################################################################################\n","\n","# You should get at least 70% accuracy\n","train_part34(model, optimizer, epochs=10)"]},{"cell_type":"code","source":[""],"metadata":{"id":"g8_3Fn6Phs7a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_0UM_JbUoy5"},"source":["## Describe what you did \n","\n","請敘述您採取的策略。"]},{"cell_type":"markdown","metadata":{"id":"jRfcEubpUoy5"},"source":["## Answer\n","\n","[[conv-relu-pool] x 3: filter channels使用3 --> 64 --> 32 --> 16 且搭配Batch normalization和使用Global average pooling來提取特徵 --> [affine] x 1: 全連接層一次收斂為10個分類，最後使用SGD with momentum 和 nesterov 來優化權重]"]},{"cell_type":"markdown","metadata":{"id":"7Whb7j_oUoy5"},"source":["## Test set -- run this only once\n","\n","請將最好的模型儲存於 `best_model`，並使用 test set 做測試。下方的 test accuracy 跟上方的 validation accuracy 有何關係？"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBCQR9IxUoy5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647784698714,"user_tz":240,"elapsed":2249,"user":{"displayName":"Cheng-Chu Chung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjjk6SqeMpTHVe4ITb3MezIB6-5WeNNKG3w1LsD=s64","userId":"17774558080259118332"}},"outputId":"96746d25-3659-4a41-9fbb-33829c4f815c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking accuracy on test set\n","Got 7527 / 10000 correct (75.27)\n"]}],"source":["best_model = model\n","check_accuracy_part34(loader_test, best_model)"]},{"cell_type":"markdown","metadata":{"id":"ox4bmusHUlvs"},"source":["---\n","# IMPORTANT\n","\n","恭喜大家完成作業！**請開啟資料夾的分享功能，並將共用連結填寫在 stanCode 作業繳交表單內！**"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"SC201_Assignment5.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b20f9c7289114ad0a97fa08da85a1b3f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db2710121d6041a0aa858638597f80a2","IPY_MODEL_5268755a9d794851b4cea7134a999f89","IPY_MODEL_f9af134660944fc881ff246775895183"],"layout":"IPY_MODEL_507c9df52a7d49fa8061bd535aeb17da"}},"db2710121d6041a0aa858638597f80a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efacad7a4b7246b7a24e0a30a4228c4e","placeholder":"​","style":"IPY_MODEL_e0ea73b3be66493a9afd107f8f6948b7","value":""}},"5268755a9d794851b4cea7134a999f89":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bce86c0fa37495ea220c47a18319cd9","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7d0d54e1f60467c8d4970a7024b1bef","value":170498071}},"f9af134660944fc881ff246775895183":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e349421b0042416e9a70e81fbf222eaf","placeholder":"​","style":"IPY_MODEL_214ddc8c2da744839c06ec3c370a780f","value":" 170499072/? [00:13&lt;00:00, 14287243.71it/s]"}},"507c9df52a7d49fa8061bd535aeb17da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efacad7a4b7246b7a24e0a30a4228c4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0ea73b3be66493a9afd107f8f6948b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bce86c0fa37495ea220c47a18319cd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7d0d54e1f60467c8d4970a7024b1bef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e349421b0042416e9a70e81fbf222eaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"214ddc8c2da744839c06ec3c370a780f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}